<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型调研-Qwen2和Qwen2VL | IAN's SITE</title><meta name="author" content="LI QIAN"><meta name="copyright" content="LI QIAN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="模型结构Qwen2-1.5B 模型结构123456789101112131415161718192021222324252627Qwen2ForCausalLM(  (model): Qwen2Model(    (embed_tokens): Embedding(151936, 1536)    (layers): ModuleList(      (0-27): 28 x Qwen2Decod">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型调研-Qwen2和Qwen2VL">
<meta property="og:url" content="https://blog.iansite.tech/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/index.html">
<meta property="og:site_name" content="IAN&#39;s SITE">
<meta property="og:description" content="模型结构Qwen2-1.5B 模型结构123456789101112131415161718192021222324252627Qwen2ForCausalLM(  (model): Qwen2Model(    (embed_tokens): Embedding(151936, 1536)    (layers): ModuleList(      (0-27): 28 x Qwen2Decod">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.iansite.tech/img/huggingface.png">
<meta property="article:published_time" content="2024-12-10T04:00:00.000Z">
<meta property="article:modified_time" content="2024-12-10T04:00:00.000Z">
<meta property="article:author" content="LI QIAN">
<meta property="article:tag" content="大模型调研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.iansite.tech/img/huggingface.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.iansite.tech/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型调研-Qwen2和Qwen2VL',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background: linear-gradient(20deg, #dfb7ac, #a99cad, #9f8fa7, #a99cad);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/huggingface_long.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">IAN's SITE</span></a><a class="nav-page-title" href="/"><span class="site-name">大模型调研-Qwen2和Qwen2VL</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大模型调研-Qwen2和Qwen2VL</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-10T04:00:00.000Z" title="发表于 2024-12-10 12:00:00">2024-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-10T04:00:00.000Z" title="更新于 2024-12-10 12:00:00">2024-12-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="Qwen2-1-5B-模型结构"><a href="#Qwen2-1-5B-模型结构" class="headerlink" title="Qwen2-1.5B 模型结构"></a><code>Qwen2-1.5B</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2ForCausalLM</span>(</span><br><span class="line">  (model): <span class="built_in">Qwen2Model</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2DecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2SdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>Qwen2ForCausalLM</code> 模型主要由两大核心组件构成：</p>
<ol>
<li><strong>模型（<code>model</code>）</strong>：基于 Transformer 的核心架构，负责处理输入 Token 并生成上下文嵌入。</li>
<li><strong>语言建模头（<code>lm_head</code>）</strong>：将模型输出的嵌入转换为对应词汇的 Logits，支持 Token 预测。</li>
</ol>
<h3 id="值得注意的地方"><a href="#值得注意的地方" class="headerlink" title="值得注意的地方"></a>值得注意的地方</h3><ol>
<li><code>Qwen2RotaryEmbedding</code>: 在注意力机制中使用<strong>旋转位置编码</strong></li>
<li><code>Qwen2MLP</code>: 在MLP中使用了 <strong>门控 MLP（Gated MLP）</strong> 架构 <ul>
<li><code>up_proj</code> 主要负责扩展特征空间，使模型能够学习更复杂的表示</li>
<li><code>gate_proj</code> 主要生成控制信息流的门控信号</li>
<li>升维过程中的</li>
<li>生成门控信号，用于调节 MLP 内部的信息流</li>
</ul>
</li>
<li><code>Qwen2RMSNorm</code>: 归一化使用 <strong>RMSNorm（均方根归一化）</strong> <ul>
<li>RMSNorm 仅基于均方根（Root Mean Square, RMS）来进行归一化，而不计算均值。</li>
<li>对于输入向量 $\mathbf{x}$，RMSNorm 的计算公式为：<script type="math/tex; mode=display">
RMSNorm(x)=γ(xRMS(x)+ϵ)+β\text{RMSNorm}(\mathbf{x}) = \gamma \left( \frac{\mathbf{x}}{\text{RMS}(\mathbf{x}) + \epsilon} \right) + \beta</script>  其中，$\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2}$，$\gamma$ 和 $\beta$ 同样是可学习参数，$\epsilon$ 防止分母为零。</li>
<li><strong>特点</strong>：<ul>
<li><strong>计算效率</strong>：RMSNorm 省去了均值的计算，降低了计算复杂度，特别是在大规模模型中，这种优化可以显著减少训练和推理时间。</li>
<li><strong>性能表现</strong>：尽管 RMSNorm 忽略了均值，但在许多实践中，它能够提供与 LayerNorm 相近甚至更好的性能，尤其是在某些特定任务或架构中。</li>
<li><strong>稳定性</strong>：RMSNorm 通过仅依赖 RMS 进行规范化，可能在某些情况下提供更稳定的梯度流动，有助于训练过程的稳定性。</li>
</ul>
</li>
</ul>
</li>
<li><code>预归一化(Pre-Norm)</code> 和 <code>后归一化(Post-Norm)</code>: <code>Qwen2DecoderLayer</code> 中有两个 RMSNorm 层: <ul>
<li>input_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制和 MLP 之前</li>
<li>post_attention_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制之后，进入 MLP 之前</li>
<li>Transformer 架构中的归一化层可以放置在不同的位置，主要有两种常见的设计：<ul>
<li><strong>后归一化（Post-Norm）</strong>：<ul>
<li>归一化层位于子层（如自注意力层或 MLP 层）之后。</li>
<li>典型的 Transformer 论文如 “Attention is All You Need” 中采用此设计。</li>
<li>缺点：在非常深的模型中，可能导致梯度消失或梯度爆炸，影响训练稳定性。</li>
</ul>
</li>
<li><strong>预归一化（Pre-Norm）</strong>：<ul>
<li>归一化层位于子层之前。</li>
<li>这种设计有助于缓解深层模型中的梯度问题，提高训练的稳定性和效率。</li>
<li>近年来，越来越多的研究和实践表明，预归一化在深层模型中表现更佳。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>强化位置信息</code>: 在进入 <code>lm_head</code> 之前再次应用 <code>rotary_emb</code></li>
<li><code>SiLU (Sigmoid Linear Unit)</code> 激活函数</li>
</ol>
<h3 id="示例流程"><a href="#示例流程" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>文本输入</strong>：提示或部分文本通过 <code>embed_tokens</code> 层进行标记化并转化为嵌入。</li>
</ul>
</li>
<li><strong>模型推理</strong>：<ul>
<li>嵌入传递到堆叠的解码层，逐层应用自注意力、前馈网络和归一化，生成上下文嵌入。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li>最终嵌入通过 <code>lm_head</code> 转换为词汇表的 Logits。</li>
<li>对 Logits 应用 Softmax 获取下一个 Token 的概率分布。</li>
<li>选择概率最高的 Token（或使用采样策略如 top-k 或 nucleus 采样）生成下一个词。</li>
</ul>
</li>
<li><strong>迭代生成</strong>：<ul>
<li>将新生成的 Token 添加到输入序列，重复该过程，直到达到终止条件（例如序列结束 Token 或最大长度）。</li>
</ul>
</li>
</ol>
<h2 id="Qwen2-VL-2B-Instruct-模型结构"><a href="#Qwen2-VL-2B-Instruct-模型结构" class="headerlink" title="Qwen2-VL-2B-Instruct 模型结构"></a><code>Qwen2-VL-2B-Instruct</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2VLForConditionalGeneration</span>(</span><br><span class="line">  (visual): <span class="built_in">Qwen2VisionTransformerPretrainedModel</span>(</span><br><span class="line">    (patch_embed): <span class="built_in">PatchEmbed</span>(</span><br><span class="line">      (proj): <span class="built_in">Conv3d</span>(<span class="number">3</span>, <span class="number">1280</span>, kernel_size=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), stride=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): <span class="built_in">VisionRotaryEmbedding</span>()</span><br><span class="line">    (blocks): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">31</span>): <span class="number">32</span> x <span class="built_in">Qwen2VLVisionBlock</span>(</span><br><span class="line">        (norm1): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (norm2): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (attn): <span class="built_in">VisionSdpaAttention</span>(</span><br><span class="line">          (qkv): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">3840</span>, bias=True)</span><br><span class="line">          (proj): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">VisionMlp</span>(</span><br><span class="line">          (fc1): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">          (act): <span class="built_in">QuickGELUActivation</span>()</span><br><span class="line">          (fc2): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (merger): <span class="built_in">PatchMerger</span>(</span><br><span class="line">      (ln_q): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">      (mlp): <span class="built_in">Sequential</span>(</span><br><span class="line">        (<span class="number">0</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">        (<span class="number">1</span>): <span class="built_in">GELU</span>(approximate=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        (<span class="number">2</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (model): <span class="built_in">Qwen2VLModel</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2VLDecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2VLSdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由上 <code>Qwen2-VL-2B-Instruct</code> 模型主要由三个核心组件组成：</p>
<ol>
<li><strong>视觉模块（<code>visual</code>）</strong>：通过基于视觉 Transformer 的架构处理并编码视觉输入。</li>
<li><strong>语言模块（<code>model</code>）</strong>：通过堆叠的解码层处理文本输入并生成输出。</li>
<li><strong>条件生成头（<code>lm_head</code>）</strong>：将语言模块的输出转化为文本生成的词概率。</li>
</ol>
<h3 id="1-视觉模块-值得注意的地方"><a href="#1-视觉模块-值得注意的地方" class="headerlink" title="1. 视觉模块-值得注意的地方"></a>1. 视觉模块-值得注意的地方</h3><ol>
<li><code>Conv3d</code>: 使用 3D 卷积覆盖非重叠的区域实现Patchify<ul>
<li>将输入的3个通道映射到1280个特征通道</li>
<li>卷积核大小(kernel_size): <code>(2, 14, 14)</code>; 步幅(stride): <code>(2, 14, 14)</code></li>
<li><code>(N, 3, D, H, W) -&gt; (N, 1280, D_out, H_out, W_out)</code><ul>
<li>1280 是每个 Token 的嵌入维度（<code>embedding dimension</code>）。</li>
<li><strong><code>D_out * H_out * W_out</code></strong> 表示生成的 Token 数量</li>
<li>每个 Token 对应于输入图像中的一个 Patch</li>
</ul>
</li>
</ul>
</li>
<li><code>VisionRotaryEmbedding</code>: 视觉特征添加位置信息</li>
<li><code>LayerNorm</code>: 图像部分使用的是LN而非RMS, 但同样是Attn前后各一个(MLP 之前)</li>
<li><code>QuickGELUActivation</code> 激活函数: 位于两个线性层之间，作为 MLP 的非线性激活函数</li>
<li><code>PatchMerger</code>: 进行视觉token数的压缩与进一步提取特征(两层MLP):<ul>
<li><strong>减少 Patch 数量</strong>：合并相邻的 Patch，减少整体的 Token 数量</li>
<li><strong>增强特征表达和对齐维度</strong>：两层MLP提取特征, 同时对齐语言模型维度</li>
<li><strong>GELU激活函数</strong></li>
</ul>
</li>
</ol>
<h3 id="2-语言模块-值得注意的地方"><a href="#2-语言模块-值得注意的地方" class="headerlink" title="2. 语言模块-值得注意的地方"></a>2. 语言模块-值得注意的地方</h3><ol>
<li><code>词表大小</code>: 151657</li>
<li><code>embed_matrix维度</code>: [151,936, 1536]</li>
<li>其余注意事项 <code>同Qwen语言模型</code></li>
</ol>
<h3 id="示例流程-1"><a href="#示例流程-1" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>视觉输入</strong>：通过视觉模块处理，将其转化为 Patch 嵌入并编码空间信息。</li>
<li><strong>文本输入</strong>：通过语言模块的嵌入层将文本转化为特征向量。</li>
</ul>
</li>
<li><strong>条件生成</strong>：<ul>
<li>将视觉和文本信息整合到模型中，生成连贯且相关的输出。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li><code>lm_head</code> 将语言模块的输出转化为词概率，生成最终文本。</li>
</ul>
</li>
</ol>
<p>qwen2vl 的一大创新就来源于对 <code>Patch</code> 的处理</p>
<h3 id="详解一下-PatchEmbed"><a href="#详解一下-PatchEmbed" class="headerlink" title="详解一下 PatchEmbed"></a>详解一下 <code>PatchEmbed</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		patch_size: <span class="built_in">int</span> = <span class="number">14</span>,</span></span><br><span class="line"><span class="params">		temporal_patch_size: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">		in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">		embed_dim: <span class="built_in">int</span> = <span class="number">1152</span>,</span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">		<span class="variable language_">self</span>.temporal_patch_size = temporal_patch_size</span><br><span class="line">		<span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">		<span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">		  </span><br><span class="line">		kernel_size = [temporal_patch_size, patch_size, patch_size]</span><br><span class="line">		<span class="variable language_">self</span>.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=<span class="literal">False</span>)</span><br><span class="line">	  </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">		target_dtype = <span class="variable language_">self</span>.proj.weight.dtype</span><br><span class="line">		hidden_states = hidden_states.view(</span><br><span class="line">		-<span class="number">1</span>, <span class="variable language_">self</span>.in_channels, <span class="variable language_">self</span>.temporal_patch_size, <span class="variable language_">self</span>.patch_size, <span class="variable language_">self</span>.patch_size</span><br><span class="line">		)</span><br><span class="line">		hidden_states = <span class="variable language_">self</span>.proj(hidden_states.to(dtype=target_dtype)).view(-<span class="number">1</span>, <span class="variable language_">self</span>.embed_dim)</span><br><span class="line">		<span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<ul>
<li><p>输入时 <code>hidden_states</code> 维度为: [tokens=5704, dim=1176]</p>
<ul>
<li>想读懂qwen2vl是怎么处理图像视频数据的, 必须搞明白 <code>processor</code> 源码是如何处理的, 尤其是这个 <code>hidden_states</code> 维度</li>
<li>维度详情为: <code>(grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size)</code></li>
<li>可以视作确定了 tokens个数, 并且确定了后续 3D卷积 处理patch<ul>
<li>相当于后面的 3D卷积 只针对一个 patch 进行, 卷出来之后 <code>时间步</code>, <code>长</code>, <code>宽</code> 维度直接为降为1</li>
</ul>
</li>
</ul>
</li>
<li><p><code>hidden_states = hidden_states.view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)</code></p>
<ul>
<li>又将 <code>hidden_states</code> 后面一个维度拆回去</li>
</ul>
</li>
<li><p><code>hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)</code></p>
<ul>
<li>这是一个 <code>dim=1176 -&gt; self.embed_dim=1280</code> 过程</li>
<li>其中 <code>self.proj</code> 是一个 3D 卷积: <ul>
<li>in_channels=3</li>
<li>embed_dim=1280</li>
<li>kernel_size=[temporal_patch_size, patch_size, patch_size]</li>
<li>stride=[temporal_patch_size, patch_size, patch_size]</li>
<li>bias=False</li>
</ul>
</li>
<li>>>> hidden_states.to(dtype=target_dtype).shape<ul>
<li>torch.Size([5704, 3, 2, 14, 14])</li>
</ul>
</li>
<li>>>> self.proj(hidden_states.to(dtype=target_dtype)).shape<ul>
<li>torch.Size([5704, 1280, 1, 1, 1])</li>
</ul>
</li>
<li>>>> self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim).shape<ul>
<li>torch.Size([5704, 1280])</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="详解一下-PatchMerger"><a href="#详解一下-PatchMerger" class="headerlink" title="详解一下 PatchMerger"></a>详解一下 <code>PatchMerger</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, context_dim: <span class="built_in">int</span>, spatial_merge_size: <span class="built_in">int</span> = <span class="number">2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	<span class="variable language_">self</span>.hidden_size = context_dim * (spatial_merge_size**<span class="number">2</span>)</span><br><span class="line">	<span class="variable language_">self</span>.ln_q = LayerNorm(context_dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">	<span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, <span class="variable language_">self</span>.hidden_size),</span><br><span class="line">		nn.GELU(),</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, dim),</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">	x = <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_q(x).view(-<span class="number">1</span>, <span class="variable language_">self</span>.hidden_size))</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>问题: 如果仔细阅读Qwen2VL的autoprocessor部分源码的话, 你会发现:</p>
<ul>
<li>tokenizer: 正常对文本部分进行分词, 使用”&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;”来进行初步的视频tokens记录</li>
<li>ImageProcessor: 按照 <code>时间步: 2, 长宽: 14x14</code> patchify</li>
<li>最终输出的inputs_id: 会将 “&lt;|image_pad|&gt;”等视觉pad变长, 但是实际长度却是 patchify 之后的 1/4, 这个原因就是来自于 <code>PatchMerger</code> 模块</li>
</ul>
<p>解答: <code>PatchMerger</code> 类用于将多个 patch 合并成一个更高维度的表示。这种合并操作会显著减少 patch 的数量。具体来说，<code>PatchMerger</code> 通过 <code>spatial_merge_size</code>（默认为 2）将相邻的 patch 合并(<code>十字相邻</code>)。例如，<code>spatial_merge_size=2</code> 表示每 2x2 的 patch 会被合并为一个新的 patch。因此，原本的 patch 数量会减少为原来的 1/4。</p>
<ul>
<li>重点在 <code>self.ln_q(x).view(-1, self.hidden_size)</code> 这行代码</li>
</ul>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><h2 id="视觉-语言-Vision-Language-VL-模型中有多个配置文件"><a href="#视觉-语言-Vision-Language-VL-模型中有多个配置文件" class="headerlink" title="视觉-语言(Vision-Language, VL)模型中有多个配置文件"></a>视觉-语言(Vision-Language, VL)模型中有多个配置文件</h2><ol>
<li><code>config.json</code><br> <code>config.json</code> 在模型初始化时被加载, 模型的主要配置文件，用于定义模型的架构和参数。它包含了模型的结构信息，使得模型能够根据这些配置正确地初始化和运行。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;architectures&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="string">&quot;Qwen2VLForConditionalGeneration&quot;</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;attention_dropout&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_start_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151652</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_end_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151653</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151654</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;image_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151655</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;video_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151656</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;hidden_act&quot;</span><span class="punctuation">:</span> <span class="string">&quot;silu&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;hidden_size&quot;</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;initializer_range&quot;</span><span class="punctuation">:</span> <span class="number">0.02</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;intermediate_size&quot;</span><span class="punctuation">:</span> <span class="number">8960</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;max_position_embeddings&quot;</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;max_window_layers&quot;</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;qwen2_vl&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_attention_heads&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_hidden_layers&quot;</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_key_value_heads&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rms_norm_eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-06</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rope_theta&quot;</span><span class="punctuation">:</span> <span class="number">1000000.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;sliding_window&quot;</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;tie_word_embeddings&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;torch_dtype&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bfloat16&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.41.2&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;use_cache&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;use_sliding_window&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_config&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">		<span class="attr">&quot;depth&quot;</span><span class="punctuation">:</span> <span class="number">32</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;embed_dim&quot;</span><span class="punctuation">:</span> <span class="number">1280</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;mlp_ratio&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;num_heads&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;in_chans&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;hidden_size&quot;</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;patch_size&quot;</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;spatial_merge_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;spatial_patch_size&quot;</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;temporal_patch_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">	<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rope_scaling&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">		<span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mrope&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;mrope_section&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span></span><br><span class="line">		<span class="punctuation">]</span></span><br><span class="line">	<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vocab_size&quot;</span><span class="punctuation">:</span> <span class="number">151936</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>Qwen2VLConfig</code> 与 <code>LlavaConfig</code> 的初始化配置类略有不同</p>
<ul>
<li><code>LlavaConfig</code> 类可以单独接收 <code>vision_config</code>, <code>text_config</code> </li>
<li><code>Qwen2VLConfig</code> 类主要接受语言模型的配置参数，并通过 <code>vision_config</code> 参数嵌套包含视觉模型的配置, 可以直接传入json</li>
</ul>
<p><code>Qwen2VLConfig</code> 接收参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Qwen2VLConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">	model_type = <span class="string">&quot;qwen2_vl&quot;</span></span><br><span class="line">	keys_to_ignore_at_inference = [<span class="string">&quot;past_key_values&quot;</span>]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		vocab_size=<span class="number">152064</span>,</span></span><br><span class="line"><span class="params">		hidden_size=<span class="number">8192</span>,</span></span><br><span class="line"><span class="params">		intermediate_size=<span class="number">29568</span>,</span></span><br><span class="line"><span class="params">		num_hidden_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		num_attention_heads=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">		num_key_value_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">		hidden_act=<span class="string">&quot;silu&quot;</span>,</span></span><br><span class="line"><span class="params">		max_position_embeddings=<span class="number">32768</span>,</span></span><br><span class="line"><span class="params">		initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">		rms_norm_eps=<span class="number">1e-05</span>,</span></span><br><span class="line"><span class="params">		use_cache=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">		tie_word_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		rope_theta=<span class="number">1000000.0</span>,</span></span><br><span class="line"><span class="params">		use_sliding_window=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		sliding_window=<span class="number">4096</span>,</span></span><br><span class="line"><span class="params">		max_window_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		attention_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">		vision_config=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		rope_scaling=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		**kwargs,</span></span><br><span class="line"><span class="params">	</span>):</span><br></pre></td></tr></table></figure></p>
<p><code>Qwen2VLConfig</code> 官方示例:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLForConditionalGeneration, Qwen2VLConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a Qwen2VL style configuration</span></span><br><span class="line">configuration = Qwen2VLConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a model from the Qwen2-VL-7B style configuration</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(configuration)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accessing the model configuration</span></span><br><span class="line">configuration = model.config</span><br></pre></td></tr></table></figure></p>
<p>自定义配置:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLConfig, Qwen2VLForConditionalGeneration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 配置文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;path/to/your/config.json&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    config_dict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Qwen2VLConfig 实例</span></span><br><span class="line">qwen2vl_config = Qwen2VLConfig(**config_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化并加载 Qwen2VL 模型</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(qwen2vl_config)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<ol>
<li><code>generation_config.json</code><br> <code>generation_config.json</code> 在调用生成方法（如 <code>generate()</code>）时被加载, 专门用于定义文本生成过程中的超参数和策略。这些配置项控制生成文本的行为，如生成长度、采样策略、温度、束搜索等, 例如:<ul>
<li><strong>生成长度</strong>：如最大生成长度（<code>max_length</code>）、最小生成长度（<code>min_length</code>）等。</li>
<li><strong>生成策略</strong>：<ul>
<li><strong>采样相关</strong>：如温度（<code>temperature</code>）、顶部K采样（<code>top_k</code>）、顶部P采样（<code>top_p</code>）等。</li>
<li><strong>束搜索</strong>：束宽度（<code>num_beams</code>）、束惩罚因子（<code>repetition_penalty</code>）等。</li>
</ul>
</li>
<li><strong>其他生成参数</strong>：如是否使用核采样（<code>do_sample</code>）、停止标记（<code>eos_token_id</code>）等。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;pad_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;do_sample&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">151643</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">0.001</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;top_k&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.37.0&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>值得注意的一个地方是, 在仅使用 <code>qwen2vl_config = Qwen2VLConfig(**config_dict)</code> 也就是 config.json 初始化模型的时候, 模型也会有推理参数, 这是 huggingface 源码中 PretrainedConfig 类初始化的时候会给一个默认的参数字典:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_generation_defaults</span>() -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">	<span class="keyword">return</span> &#123;</span><br><span class="line">		<span class="string">&quot;max_length&quot;</span>: <span class="number">20</span>,</span><br><span class="line">		<span class="string">&quot;min_length&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;do_sample&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;early_stopping&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;num_beams&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;num_beam_groups&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;diversity_penalty&quot;</span>: <span class="number">0.0</span>,</span><br><span class="line">		<span class="string">&quot;temperature&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;top_k&quot;</span>: <span class="number">50</span>,</span><br><span class="line">		<span class="string">&quot;top_p&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;typical_p&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;repetition_penalty&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;length_penalty&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;no_repeat_ngram_size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;encoder_no_repeat_ngram_size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;bad_words_ids&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;num_return_sequences&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;output_scores&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;return_dict_in_generate&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;forced_bos_token_id&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;forced_eos_token_id&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;remove_invalid_values&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;exponential_decay_length_penalty&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;suppress_tokens&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;begin_suppress_tokens&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p><code>vocab.json</code><br> <code>vocab.json</code> 文件主要用于定义分词器的词汇表。它包含了模型可以识别和处理的所有词汇（tokens）及其对应的唯一标识符（IDs）。一些特殊tokens标记一般不会出现在这里    </p>
</li>
<li><p><code>tokenizer_config.json</code><br> <code>tokenizer_config.json</code> 文件用于存储分词器的高层配置参数。这些参数影响分词器的行为和处理方式, 如填充方式(<code>padding_side</code>)、添加特殊标记(<code>add_special_tokens</code>)、最大序列长度(<code>model_max_length</code>)等. 但不涉及具体的词汇映射或分词逻辑:</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&quot;add_prefix_space&quot;: false,</span><br><span class="line">	<span class="string">&quot;added_tokens_decoder&quot;</span>: &#123;</span><br><span class="line">		&quot;<span class="number">151643</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151644</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151645</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151646</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|object_ref_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151647</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|object_ref_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151648</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|box_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151649</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|box_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151650</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|quad_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151651</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|quad_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151652</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151653</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151654</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151655</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|image_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151656</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|video_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;additional_special_tokens&quot;: [<span class="string">&quot;&lt;|im_start|&gt;&quot;</span>, <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>, <span class="string">&quot;&lt;|object_ref_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|object_ref_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|box_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|box_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|quad_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|quad_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_pad|&gt;&quot;</span>,<span class="string">&quot;&lt;|image_pad|&gt;&quot;</span>,<span class="string">&quot;&lt;|video_pad|&gt;&quot;</span>],</span><br><span class="line">	<span class="string">&quot;bos_token&quot;</span>: null,</span><br><span class="line">	<span class="string">&quot;chat_template&quot;</span>: <span class="string">&quot;&#123;% set image_count = namespace(value=0) %&#125;&#123;% set video_count = namespace(value=0) %&#125;&#123;% for message in messages %&#125;&#123;% if loop.first and message[&#x27;role&#x27;] != &#x27;system&#x27; %&#125;&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&#123;% endif %&#125;&lt;|im_start|&gt;&#123;&#123; message[&#x27;role&#x27;] &#125;&#125;\n&#123;% if message[&#x27;content&#x27;] is string %&#125;&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&#123;% else %&#125;&#123;% for content in message[&#x27;content&#x27;] %&#125;&#123;% if content[&#x27;type&#x27;] == &#x27;image&#x27; or &#x27;image&#x27; in content or &#x27;image_url&#x27; in content %&#125;&#123;% set image_count.value = image_count.value + 1 %&#125;&#123;% if add_vision_id %&#125;Picture &#123;&#123; image_count.value &#125;&#125;: &#123;% endif %&#125;&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;&#123;% elif content[&#x27;type&#x27;] == &#x27;video&#x27; or &#x27;video&#x27; in content %&#125;&#123;% set video_count.value = video_count.value + 1 %&#125;&#123;% if add_vision_id %&#125;Video &#123;&#123; video_count.value &#125;&#125;: &#123;% endif %&#125;&lt;|vision_start|&gt;&lt;|video_pad|&gt;&lt;|vision_end|&gt;&#123;% elif &#x27;text&#x27; in content %&#125;&#123;&#123; content[&#x27;text&#x27;] &#125;&#125;&#123;% endif %&#125;&#123;% endfor %&#125;&lt;|im_end|&gt;\n&#123;% endif %&#125;&#123;% endfor %&#125;&#123;% if add_generation_prompt %&#125;&lt;|im_start|&gt;assistant\n&#123;% endif %&#125;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;clean_up_tokenization_spaces&quot;</span>: false,</span><br><span class="line">	<span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;padding_side&quot;</span>: <span class="string">&quot;left&quot;</span>,</span><br><span class="line">	<span class="string">&quot;errors&quot;</span>: <span class="string">&quot;replace&quot;</span>,</span><br><span class="line">	<span class="string">&quot;model_max_length&quot;</span>: <span class="number">32768</span>,</span><br><span class="line">	<span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;split_special_tokens&quot;</span>: false,</span><br><span class="line">	<span class="string">&quot;tokenizer_class&quot;</span>: <span class="string">&quot;Qwen2Tokenizer&quot;</span>,</span><br><span class="line">	<span class="string">&quot;unk_token&quot;</span>: null</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tokenizer.json</code><br><code>tokenizer.json</code> 是一个综合性文件，通常包含了分词器的完整配置和分词逻辑。它不仅包含 <code>vocab.json</code> 和 <code>tokenizer_config.json</code> 的内容(但<code>tokenizer.json</code>中的add_tokens可能没有<code>tokenizer_config.json</code>中全)，还包括分词器的具体实现细节，如分词合并规则、正则表达式等, 结合 <code>tokenizer_config.json</code> 的内容，提供完整的分词器配置</p>
</li>
</ol>
<h3 id="vocab-json-tokenizer-json-和-tokenizer-config-json"><a href="#vocab-json-tokenizer-json-和-tokenizer-config-json" class="headerlink" title="vocab.json, tokenizer.json 和 tokenizer_config.json"></a><code>vocab.json</code>, <code>tokenizer.json</code> 和 <code>tokenizer_config.json</code></h3><ul>
<li><strong><code>vocab.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>vocab.json</code> 提供了词汇到ID的基础映射，是分词器不可或缺的一部分。</li>
<li><code>tokenizer.json</code> 将 <code>vocab.json</code> 嵌入其中，并结合分词规则（如BPE的合并规则）和行为参数，形成一个完整的分词器定义。</li>
</ul>
</li>
<li><strong><code>tokenizer_config.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>tokenizer_config.json</code> 专注于高层次的分词器配置参数，控制分词器的整体行为。</li>
<li><code>tokenizer.json</code> 不仅包含 <code>tokenizer_config.json</code> 的内容，还包括具体的分词逻辑和词汇表，是一个更全面的配置文件。</li>
</ul>
</li>
</ul>
<h2 id="Qwen2-1-5B-config"><a href="#Qwen2-1-5B-config" class="headerlink" title="Qwen2-1.5B config"></a><code>Qwen2-1.5B</code> config</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Qwen2Config &#123;</span><br><span class="line">  &quot;_attn_implementation_autoset&quot;: true,</span><br><span class="line">  <span class="string">&quot;_name_or_path&quot;</span>: <span class="string">&quot;/mnt/nas/ianli/models/Qwen2-1.5B&quot;</span>,</span><br><span class="line">  <span class="string">&quot;architectures&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;Qwen2ForCausalLM&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;attention_dropout&quot;</span>: <span class="number">0.0</span>,</span><br><span class="line">  <span class="string">&quot;bos_token_id&quot;</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">&quot;eos_token_id&quot;</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;silu&quot;</span>,</span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: <span class="number">1536</span>,</span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: <span class="number">8960</span>,</span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: <span class="number">131072</span>,</span><br><span class="line">  <span class="string">&quot;max_window_layers&quot;</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">&quot;model_type&quot;</span>: <span class="string">&quot;qwen2&quot;</span>,</span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">&quot;num_key_value_heads&quot;</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">&quot;rms_norm_eps&quot;</span>: <span class="number">1</span>e-<span class="number">06</span>,</span><br><span class="line">  <span class="string">&quot;rope_scaling&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;rope_theta&quot;</span>: <span class="number">1000000.0</span>,</span><br><span class="line">  <span class="string">&quot;sliding_window&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;tie_word_embeddings&quot;</span>: true,</span><br><span class="line">  <span class="string">&quot;torch_dtype&quot;</span>: <span class="string">&quot;bfloat16&quot;</span>,</span><br><span class="line">  <span class="string">&quot;transformers_version&quot;</span>: <span class="string">&quot;4.46.3&quot;</span>,</span><br><span class="line">  <span class="string">&quot;use_cache&quot;</span>: true,</span><br><span class="line">  <span class="string">&quot;use_sliding_window&quot;</span>: false,</span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: <span class="number">151936</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.iansite.tech">LI QIAN</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.iansite.tech/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/">https://blog.iansite.tech/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.iansite.tech" target="_blank">IAN's SITE</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/">大模型调研</a></div><div class="post-share"><div class="social-share" data-image="/img/huggingface.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/04/Huggingface-PreTrain-01-MLM/" title="Huggingface-PreTrain-01-MLM"><img class="cover" src="/img/huggingface.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Huggingface-PreTrain-01-MLM</div></div><div class="info-2"><div class="info-item-1">整理中 </div></div></div></a><a class="pagination-related" href="/2024/12/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80-01-GPT%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91%E5%92%8C%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97,%20%E4%BB%A5%E5%8F%8ASFT%E6%83%85%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%84%E7%90%86/" title="大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理"><img class="cover" src="/img/huggingface.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理</div></div><div class="info-2"><div class="info-item-1"> 模型结构与序列长度的关系 首先，tokens的长度并不会从定义上影响Transformer网络的结构。Transformer的计算本质是对Embedding向量进行处理，因此无论输入的序列长度（N）是多少，Transformer的结构（层数、头数、隐藏维度等）并不发生变化。  不过，序列长度会影响计算量：  在自注意力（Self-Attention）机制中，需要计算长度为N的序列中每个token与其他N-1个token之间的关联权重，复杂度为$O(N^2)$，因此序列越长，计算越耗时。 如果使用的是绝对位置编码，那么位置编码的定义方式可能受到序列长度的影响（例如编码范围、位置映射方式等）。相对位置编码就不受此直接限制。   数据流向（Data Flow） 以一个简单的案例为例：假设输入数据的形状为 (batch_size=1, sequence_length=10)，模型的embedding维度为768，词汇表大小为50,000。  首先，原始输入 tokens 会通过 Embedding Layer 映射为 (1, 10, 768)...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LI QIAN</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/li508q"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/li508q" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:liqian508@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Qwen2-1-5B-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">Qwen2-1.5B 模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9"><span class="toc-number">1.1.1.</span> <span class="toc-text">值得注意的地方</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">示例流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Qwen2-VL-2B-Instruct-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">Qwen2-VL-2B-Instruct 模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9D%97-%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. 视觉模块-值得注意的地方</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9D%97-%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. 语言模块-值得注意的地方</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E6%B5%81%E7%A8%8B-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">示例流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%A6%E8%A7%A3%E4%B8%80%E4%B8%8B-PatchEmbed"><span class="toc-number">1.2.4.</span> <span class="toc-text">详解一下 PatchEmbed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%A6%E8%A7%A3%E4%B8%80%E4%B8%8B-PatchMerger"><span class="toc-number">1.2.5.</span> <span class="toc-text">详解一下 PatchMerger</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.</span> <span class="toc-text">配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%86%E8%A7%89-%E8%AF%AD%E8%A8%80-Vision-Language-VL-%E6%A8%A1%E5%9E%8B%E4%B8%AD%E6%9C%89%E5%A4%9A%E4%B8%AA%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.1.</span> <span class="toc-text">视觉-语言(Vision-Language, VL)模型中有多个配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vocab-json-tokenizer-json-%E5%92%8C-tokenizer-config-json"><span class="toc-number">2.1.1.</span> <span class="toc-text">vocab.json, tokenizer.json 和 tokenizer_config.json</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Qwen2-1-5B-config"><span class="toc-number">2.2.</span> <span class="toc-text">Qwen2-1.5B config</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/05/Huggingface-PEFT-01-Prompt-Tuning/" title="Huggingface-PEFT-01-Prompt-Tuning"><img src="/img/huggingface.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface-PEFT-01-Prompt-Tuning"/></a><div class="content"><a class="title" href="/2025/01/05/Huggingface-PEFT-01-Prompt-Tuning/" title="Huggingface-PEFT-01-Prompt-Tuning">Huggingface-PEFT-01-Prompt-Tuning</a><time datetime="2025-01-05T04:00:00.000Z" title="发表于 2025-01-05 12:00:00">2025-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/04/Huggingface-PreTrain-02-CausalLM(GPT)/" title="Huggingface-PreTrain-02-CausalLM(GPT)"><img src="/img/huggingface.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface-PreTrain-02-CausalLM(GPT)"/></a><div class="content"><a class="title" href="/2025/01/04/Huggingface-PreTrain-02-CausalLM(GPT)/" title="Huggingface-PreTrain-02-CausalLM(GPT)">Huggingface-PreTrain-02-CausalLM(GPT)</a><time datetime="2025-01-04T06:00:00.000Z" title="发表于 2025-01-04 14:00:00">2025-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/04/Huggingface-PreTrain-01-MLM/" title="Huggingface-PreTrain-01-MLM"><img src="/img/huggingface.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Huggingface-PreTrain-01-MLM"/></a><div class="content"><a class="title" href="/2025/01/04/Huggingface-PreTrain-01-MLM/" title="Huggingface-PreTrain-01-MLM">Huggingface-PreTrain-01-MLM</a><time datetime="2025-01-04T04:00:00.000Z" title="发表于 2025-01-04 12:00:00">2025-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/" title="大模型调研-Qwen2和Qwen2VL"><img src="/img/huggingface.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型调研-Qwen2和Qwen2VL"/></a><div class="content"><a class="title" href="/2024/12/10/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-01-Qwen2%E5%92%8CQwen2VL/" title="大模型调研-Qwen2和Qwen2VL">大模型调研-Qwen2和Qwen2VL</a><time datetime="2024-12-10T04:00:00.000Z" title="发表于 2024-12-10 12:00:00">2024-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80-01-GPT%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91%E5%92%8C%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97,%20%E4%BB%A5%E5%8F%8ASFT%E6%83%85%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%84%E7%90%86/" title="大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理"><img src="/img/huggingface.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理"/></a><div class="content"><a class="title" href="/2024/12/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80-01-GPT%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91%E5%92%8C%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97,%20%E4%BB%A5%E5%8F%8ASFT%E6%83%85%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%84%E7%90%86/" title="大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理">大模型基础-01-GPT的数据流向和损失计算, 以及SFT情景下的处理</a><time datetime="2024-12-01T04:00:00.000Z" title="发表于 2024-12-01 12:00:00">2024-12-01</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By LI QIAN</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>