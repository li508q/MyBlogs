---
title: 因果模型-03-因果森林
date: 2024-10-15 12:00:00
updated: 2024-10-15 12:00:00
tags: 因果模型
categories: 算法
---

[主要参考](https://www.cnblogs.com/zhouyc/p/18144726)
## 总结

### 处理
1. 提出了广义随机森林(GRF)这一框架。
2. 这个广义随机森林定义了一个矩估计，得到一个矩条件。
	- 矩估计的核心目标是找到一组参数 $\theta$，使得基于这些参数的某种条件期望（矩条件）成立
	- $\mathbb{E}[\cdot] = 0$ 就是一个矩条件
3. 通过最小化 Score Function，我们可以找到符合矩条件的参数。
	- $\psi_{\theta(x), v(x)}(O_i)$ 即该矩条件下 Score Function，作用是量化模型参数 $\theta$ 的误差或偏差
4. 分裂框架。
	- **父节点优化目标**：参数估计遵循 `极小化 Score Function`，为整个节点的数据提供了一个整体的拟合基准。使用全局信息，使模型能够在当前节点上有一个合理的参数估计，并为子节点分裂提供必要参数。
	- **子节点分裂准则**：试图将数据划分到两个更同质的子节点中，使得每个子节点内的数据在参数估计上更加准确。利用局部信息（即子节点内的数据）来进一步精细化参数估计，从而减少整体的估计误差。

### 效果
1. 通过在父节点上最小化 Score Function 保证广义随机森林为整个节点提供一个高质量的参数估计基准，以及节点处理效应的无偏估计
2. 最大化子节点间的异质性 $\Delta(C_1, C_2)$ 确保了分裂后的子节点在参数空间上具有显著差异，使得模型能够更准确地识别对不同处理(Treatment)响应不同的个体群体，提升因果效应估计精度。

## 最小化目标——定义矩估计

定义一下矩估计参数表达式：
$$\mathbb{E}[\psi_{\theta(x), v(x)}(O_i) | X = x] = 0$$
其中，$\psi$ 是 score function，也就是 measure metric，$\theta$ 是我们不得不去计算的参数，比如 tree 里面的各项参数如特征 threshold，叶子节点估计值..etc, $v$ 则是一个可选参数。$O$ 表示和计算相关的值，比如监督信号。像 response 类的模型，$O_i = Y_i$，像 causal 模型，$O_i = Y_i, W_i$ 表示某种 treatment。  
该式在实际优化参数的时候，等价于最小化：
$$(\hat{\theta}(x), v(x)) \in \arg \min_{\theta, v} \| \sum \alpha_i(x) \psi_{\theta, v}(O_i) \|_2$$
其中，$\alpha$ 是一种权重，当然，这里也可以理解为树的权重，假设总共需要学习 $B$ 棵树：
$$\alpha_i(x) = \frac{1}{B} \sum_{b=1}^B \alpha_{bi}(x)$$
$$\alpha_{bi}(x) = \frac{1((x \in L_b(x)))}{|L_b(x)|}$$
其中，$L_b(x)$ 表示叶子节点里的样本。本质上，这个权重表示的是：训练样本和推理或者测试样本的相似度，因为如果某个样本 $x_i$ 落入叶子 $L_b$，且我们可以认为叶子节点内的样本同质的情况下，那么可以认为这个样本和当前落入的 tree 有相似性。

当然，按照这个公式，如果 $L_b$ 很大，说明进入这个叶子的训练样本很多，意味着没有完全分配给这棵树的权重就低，反之亦然。

## 分裂准则框架

### 父节点优化目标

对于每棵树，父节点 $P$ 通过最优化下式进行分裂，其中，$\mathcal{J}$ 表示 train set：

$$
(\hat{\theta}_P, \hat{\nu}_P) \in \arg \min_{\theta, \nu} \left\{ \left\| \sum_{\{i \in \mathcal{J} : X_i \in P\}} \psi_{\theta, \nu}(O_i) \right\|_2 \right\}.
$$

### 子节点分裂标准

分裂后形成的 2 个子节点标准为**通过最小化估计值与真实值间的误差平方**：
$$
\text{err}(C_1, C_2) = \sum_{j=1,2} \mathbb{P}[X \in C_j | X \in P] \mathbb{E} \left[ \left( \hat{\theta}_{C_j}(\mathcal{J}) - \theta(X) \right)^2 \middle| X \in C_j \right]
$$
等价于**最大化节点间的异质性**：
$$
\Delta(C_1, C_2) := n_{C_1}n_{C_2}/n_P^2 \left( \hat{\theta}_{C_1}(\mathcal{J}) - \hat{\theta}_{C_2}(\mathcal{J}) \right)^2
$$

### 实际计算方式

但是 $\theta$ 参数比较难优化，交给梯度下降：
$$
\hat{\theta}_C = \hat{\theta}_P - \frac{1}{|\{i : X_i \in C\}|} \sum_{\{i : X_i \in C\}} \xi^\top A_P^{-1} \psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i)
$$
其中，$\hat{\theta}_P$ 通过 $(\hat{\theta}(x), v(x)) \in \arg \min_{\theta, v} \| \sum \alpha_i(x) \psi_{\theta, v}(O_i) \|_2$ 获得，$A_P$ 为 score function 的梯度
$$
A_P = \frac{1}{|\{i : X_i \in P\}|} \sum_{\{i : X_i \in P\}} \nabla \psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i),
$$
梯度计算部分包含 2 个 step：
- **step1**: labeling-step 得到一个 pseudo-outcomes  
  $$\rho_i = -\xi^\top A_P^{-1} \psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i) \in \mathbb{R}.$$
- **step2**: 回归阶段，用这个 pseudo-outcomes 作为信号，传递给 split 函数，最终是最大化下式指导节点分割：
$$
  \Delta(C_1, C_2) = \sum_{j=1}^2 \frac{1}{|\{i : X_i \in C_j\}|} \left( \sum_{\{i : X_i \in C_j\}} \rho_i \right)^2
$$

## 关于森林中树权重的补充

广义随机森林（Generalized Random Forest）中关于**权重估计**的一个步骤。其目标是在模型训练和预测中，基于叶节点中的“共现频率”计算每个数据点的权重，以便更好地解释模型的局部估计。

具体来说，步骤如下：

1. **定义局部权重** $\alpha_{bi}(x)$：
   - $\alpha_{bi}(x)$ 表示数据点 $x$ 在第 $b$ 棵树的叶节点中的权重。
   - 公式 $\alpha_{bi}(x) = \frac{1\{x_i \in l_b(x)\}}{|l_b(x)|}$ 表示，如果数据点 $x_i$ 落入第 $b$ 棵树的叶节点 $l_b(x)$，那么 $x_i$ 对 $x$ 的权重为 $1/|l_b(x)|$，其中 $|l_b(x)|$ 表示叶节点中包含的样本数。
   - 这个权重反映了数据点在同一个叶节点中出现的频率（即“共现频率”），用于衡量测试数据和训练数据在该叶节点上的相似性。

2. **综合权重** $\alpha_i(x)$：
   - $\alpha_i(x)$ 是针对所有树的综合权重，是通过对每棵树的局部权重 $\alpha_{bi}(x)$ 取平均得到的，即 $\alpha_i(x) = \frac{1}{B} \sum_{b=1}^B \alpha_{bi}(x)$，其中 $B$ 是树的数量。
   - 这个综合权重 $\alpha_i(x)$ 代表了在整个森林中，测试数据与训练数据在叶节点中的“共现频率”的平均值。权重越大，说明测试数据和训练数据在树结构中的相似性越强。

**总结**：
这种权重计算方式的目的是在广义随机森林中，通过这种基于“共现频率”的权重，衡量测试数据与训练数据的相似性，从而在预测时使模型能够关注那些和测试数据具有相似结构的训练数据。这种方法增强了模型在局部上的解释性。